{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70fe7d1-b685-421f-81e8-cdf1fe2f63cf",
   "metadata": {},
   "source": [
    "### Implementation From Scratch of Deep N-Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e180b7-feb2-4f22-a80a-e3810247181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils import sigmoid, relu, relu_backward, sigmoid_backward\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10961a25-1d26-4cf8-a1a8-724baab8a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eced3f-beb0-47a1-9660-f4482d814815",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "**Steps Needed to Implmenet**\n",
    "\n",
    "1. Intialize Parameters (W,b)\n",
    "2. Implement Forward Prop Layers in loop (N-Layers)\n",
    "3. Compute Loss function\n",
    "4. Carry out back prop to get new parameters by calculating derivatives at every layer\n",
    "5. Update parameters using Gradient Descent\n",
    "6. Make Prediction\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f4085fa-6e7a-451a-b5ff-43e49e5ead19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. Intilaie Parameters\n",
    "def intialize_parameters(layers_dim):\n",
    "    # layer_dims : array of dims of each layer\n",
    "    parameters = {}\n",
    "    L = len(layers_dim) # number of layers in network\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l)] = np.random.randn(layers_dim[l],layers_dim[l-1]) * 0.01\n",
    "        parameters[\"b\"+str(l)] = np.zeros((layers_dim[l],1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "008e2bc7-3c35-4bee-9dfb-97389d61d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. Implement Forward prop - linear part\n",
    "def linear_forward_prop(A,W,b):\n",
    "    Z = np.dot(A,W) + b\n",
    "    cache = (A,W,b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e4b2bbf-22b7-4e44-b731-14e5879698f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. Implement Forward prop - non linear (activation) part\n",
    "def single_layer_forward_prop(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward_prop(A_prev, W, b)\n",
    "        A,activation_cache = sigmoid(Z) # sigmoid is implmented in dnn_utils.py\n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward_prop(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z) # sigmoid is implmented in dnn_utils.py\n",
    "    cache =   (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a3f873c-15eb-4fd0-a0a5-d8fa149f266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward_prop(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A, cache = single_layer_forward_prop(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = single_layer_forward_prop(A,parameters['W'+str(L)],parameters['b'+str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e55aad2f-cb1f-4782-abd0-3514a386340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Compute Cost Function (cross-entropy in this case)\n",
    "def computer_cost(AL,Y):\n",
    "\n",
    "    m = Y.shape[1] # total examples\n",
    "    cost += np.multiply(Y,np.log(AL)) + np.multiply((1-Y,np.log(1-AL)))\n",
    "    cost /= m\n",
    "    np.squeeze(cost)\n",
    "\n",
    "    return cost\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f72bee9-bf06-4089-a100-c36830854eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Back Prop - calculate gradient of loss func wrt paramters \n",
    "def linear_backward(dZ,cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ,A_prev.T) / m\n",
    "    db = (np.sum(dZ,axis=1,keepdims=True)) /m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4de51cf0-2d2d-46c6-84c7-084ef3ab950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA,cache,activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a91310de-5740-4659-89ca-b3efc0dbd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_backward_prop(AL,Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    dAL = -(np.divide(Y,AL) - np.divide(1-Y,1-AL))\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev, dW, db = linear_activation_backward(dAL, current_cache,'sigmoid')\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\"+str(l)],current_cache,'relu')\n",
    "        grads[\"dA\"+str(l)] = dA_prev\n",
    "        grads[\"dW\"+str(l+1)] = dW\n",
    "        grads[\"db\"+str(l+1)] = db\n",
    "    \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "004226b0-1f68-4bab-81e0-5ef50f18a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Update Paramters \n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    paramters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] += -learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] += -learning_rate*grads['db'+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ceef09-a494-4ae7-bd0d-cab68c163051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
